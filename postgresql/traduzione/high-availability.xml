<?xml version="1.0" encoding="UTF-8"?>
<!-- $PostgreSQL$ -->

<chapter id="high-availability">
 <title>Alta disponibilità, bilanciamento di carico e replica</title>

 <indexterm><primary>alta disponibilità</primary></indexterm>
 <indexterm><primary>failover</primary></indexterm>
 <indexterm><primary>replica</primary></indexterm>
 <indexterm><primary>bilanciamento di carico</primary></indexterm>
 <indexterm><primary>clustering</primary></indexterm>
 <indexterm><primary>partizionamento dei dati</primary></indexterm>

 <para>
I server database possono lavorare insieme per permettere ad un secondo  
server di prenderne il posto se il server primario fallisce (alta
disponibilità), o per permettere a diversi computer di servire gli 
stessi dati (bilanciamento di carico). Idealmente, i server database 
potrebbero lavorare insieme ininterrottamente. I server web che servono
pagine web statiche possono essere combinati abbastanza facilmente
semplicemente bilanciando le richieste web su molteplici macchine. 
Infatti, server database in sola lettura possono essere combinati in maniera
relativamente semplice. Sfortunatamente, la maggior parte dei server database
hanno un insieme di richieste di lettura e scrittura, e i server in lettura/scrittura
sono molto difficili da combinare. Questo perchè sebbene i dati in sola lettura 
devono essere posizionati su ogni server solo una volta, una scrittura 
ad un qualsiasi server deve essere propagata a tutti i server così che le 
letture future su quei server ritornino risultati consistenti. 
 </para>

 <para>
Questo problema di sincronizzazione è la difficoltà fondamentale perchè
i server lavorino insieme. Dato che non c'è una singola soluzione
che elimina l'impatto del problema di sincronizzazione per tutti i casi d'uso, 
ci sono diverse possibilità. Ogni soluzione indirizza questo problema in modo
differente, e minimizza il suo impatto per un carico di lavoro specifico.
 </para>

 <para>
Alcune soluzioni realizzano la sincronizzazione permettendo solo 
ad un server di modificare i dati. I server che possono modificare i dati 
vengono chiamati server read/write, <firstterm>master</firstterm> o
<firstterm>primary</firstterm>.
I server che tengono traccia dei cambiamenti nel master vengono chiamati 
<firstterm>standby</firstterm>
o server <firstterm>slave</firstterm>. Un server standby a cui non ci si può connettere  
finchè non viene promosso a server master viene chiamato server <firstterm>warm
standby</firstterm>, ed uno che può accettare connessioni e servire query in sola lettura
viene chiamato server <firstterm>hot standby</firstterm>.
 </para>

 <para>
Alcune soluzioni sono sincrone, 
che significa che una transazione che non modifica i dati non si considera
sottoposta a commit finchè tutti i server non hanno fatto il commit della transazione.
Questo garantisce un failover che non perderà nessun dato e i server 
con bilanciamento di carico restituiranno risultati consistenti qualsiasi 
server verrà interrogato. Al contrario, soluzioni asincrone permettono 
un po' di ritardo tra il momento di una commit e la sua propagazione agli altri server, 
aprendo la possibilità che alcune transazioni potrebbero essere perse
nel passaggio a un server di backup, e che i server con carico bilanciatio
potrebbero ritornare risultati leggermenti non aggiornati. La comunicazione
asincrona viene usata quando quella sincrona sarebbe troppo lenta.
 </para>

 <para>
Le soluzioni possono anche essere categorizzate dalla loro granularità. Alcune 
soluzioni possono trattare un intero server di database, mentre altre permettono
di avere un controllo a livello di tabella o di singolo database.
 </para>

 <para>
Le prestazioni devono essere tenute in considerazione prima di prendere 
qualsiasi decisione. Di solito c'è un trade-off tra le funzionalità 
e le prestazioni. Per esempio, una soluzione completamente sincrona 
su una rete lenta potrebbe far diminuire le prestazioni di più della metà,
mentre una asincrona potrebbe avere un impatto minimo sulle prestazioni.
 </para>

 <para>
Il promemoria di questa sezione evidenzia diverse soluzioni di failover, replica, 
e bilanciamento di carico. Anche un  <ulink
url="http://www.postgres-r.org/documentation/terms">glossario</ulink> 
è disponibile.
 </para>

 <sect1 id="different-replication-solutions">
 <title>Confronto di diverse solutioni</title>

 <variablelist>

  <varlistentry>
   <term>Shared Disk Failover</term>
   <listitem>

    <para>
Shared disk failover evita l'overhead di sincronizzazione avendo una sola copia  
del database. Usa un singolo array di disco che è condiviso da molteplici server.
Se il server database principale fallisce, il server standby
è capace di montare e avviare il database sebbene stesse recuperando da un crash.
Questo permette un failover rapido senza perdita di dati.
    </para>

    <para>
     Shared hardware functionality is common in network storage devices.
     Using a network file system is also possible, though care must be
     taken that the file system has full <acronym>POSIX</acronym> behavior (see <xref
     linkend="creating-cluster-nfs"/>).  
Una limitazione significativa di questo metodo è che se l'array di disco condiviso
fallisce o si corrompe, i server primario e standby diventano entrambi non funzionanti.
Un'altra questione è che il server standby non accede mai l'immagazzinamento condiviso 
mentre il server primario è in esecuzione.
    </para>

   </listitem>
  </varlistentry>

  <varlistentry>
   <term>File System (Block-Device) Replication</term>
   <listitem>

    <para>
     A modified version of shared hardware functionality is file system
     replication, where all changes to a file system are mirrored to a file
     system residing on another computer.  
La sola restrizione è che 
il mirroring deve essere fatto in modo da assicurare che il server standby abbia
una copia consistente del file system - nello specifico, le scritture 
sullo standby devono essere fatte nello stesso ordine di quelle sul master.
<productname>DRBD</productname> è una soluzione popolare di replica con file system
per Linux.
    </para>

<!--
https://forge.continuent.org/pipermail/sequoia/2006-November/004070.html

Oracle RAC is a shared disk approach and just send cache invalidations
to other nodes but not actual data. As the disk is shared, data is
only committed once to disk and there is a distributed locking
protocol to make nodes agree on a serializable transactional order.
-->

   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Warm and Hot Standby Using Point-In-Time Recovery (<acronym>PITR</acronym>)</term>
   <listitem>

    <para>
I server warm e hot standby possono essere tenuti aggiornati leggendo  
uno stream di record di log write-ahead (<acronym>WAL</acronym>).
Se il server principale fallisce, lo standby contiene
praticamente tutti i dati del server principale, e può essere reso velocemente
il nuovo server database master. Quest'operazione è asincrona e
può essere fatta solo per l'intero server database.
    </para>
    <para>
Un server stanby PITR può essere implementato usando il log shipping basato sui file
(<xref linkend="warm-standby"/>) o la replica streaming (si veda
<xref linkend="streaming-replication"/>), o una combinazione di entrambi. Per  
informazioni su hot standby, si veda <xref linkend="hot-standby"/>.
    </para>
   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Replica master-slave basata sui trigger</term>
   <listitem>

    <para>
Un sistema di replica master-standby manda tutte le query che modificano i dati
al server master. Il server master manda in modo asincrono
i cambiamenti dei dati al server standby. Lo standby può rispondere
con query in sola lettura mentre il server master è in esecuzione. Il server
standby è ideale per query di data warehouse.
    </para>

    <para>
<productname>Slony-I</productname> è un esempio di questo tipo di replica, con granularità
a livello di tabella, e supporto per molti server standby. Dato che aggiorna
il server standby in modo asincrono, c'è una possibile perdita di dati 
durante il fail over.
    </para>
   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Statement-Based Replication Middleware</term>
   <listitem>

    <para>
     With statement-based replication middleware, a program intercepts
     every SQL query and sends it to one or all servers.  Each server
     operates independently.  Read-write queries are sent to all servers,
     while read-only queries can be sent to just one server, allowing
     the read workload to be distributed.
    </para>

    <para>
     If queries are simply broadcast unmodified, functions like
     <function>random()</function>, <function>CURRENT_TIMESTAMP</function>, and
     sequences can have different values on different servers.
     This is because each server operates independently, and because
     SQL queries are broadcast (and not actual modified rows).  If
     this is unacceptable, either the middleware or the application
     must query such values from a single server and then use those
     values in write queries.  Another option is to use this replication
     option with a traditional master-standby setup, i.e. data modification
     queries are sent only to the master and are propagated to the
     standby servers via master-standby replication, not by the replication
     middleware.  Care must also be taken that all
     transactions either commit or abort on all servers, perhaps
     using two-phase commit (<xref linkend="sql-prepare-transaction"/>
     and <xref linkend="sql-commit-prepared"/>.
     <productname>Pgpool-II</productname> and <productname>Sequoia</productname> are examples of
     this type of replication.
    </para>
   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Replica multimaster asincrona</term>
   <listitem>

    <para>
Per server che non sono connessi regolarmente, come portatili o 
server remoti, mantenere la consistenza dei dati tra i server è una 
sfida. Usando la replica multimaster asincrona, ogni server funziona
indipendentemente, e comunica periodicamente con gli altri server
per identificare transazioni che sono in conflittto. I conflitti 
possono essere risolti da utenti o da regole di risoluzione dei conflitti.
Bucardo è un esempio di questo tipo di replica.
    </para>
   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Replica multimaster sincrona</term>
   <listitem>

    <para>
Nella replica multimaster sincrona, ogni server può accettare 
richieste di scrittura, e i dati modificati vengono trasmessi dal server originale 
ad ogni altro server prima di ogni commit di transazione. 
Attività di scrittura pesante può causare un'eccessiva attività di locking, 
generando prestazioni scarse. Infatti, le prestazioni di scrittura spesso 
sono peggiori rispetto a quelle di un singolo server. Le richieste di lettura 
possono essere mandate a qualsiasi server. Alcune implementazioni usano dischi condivisi
per ridurre l'overhead di comunicazione. La replica multimaster sincrona
è la migliore per carichi di lavoro che sono per la maggior parte in lettura, 
sebbene il suo maggior vantaggio è che qualsiasi server accetta rischieste di scrittura - 
non c'è bisogno di partizionale i carichi di lavoro tra i server master e standby,
e dato che i cambiamenti dei dati vengono mandati da un server all'altro, 
non c'è problema con funzioni non deterministiche come 
<function>random()</function>.
    </para>

    <para>
<productname>PostgreSQL</productname> non offre questo tipo di replica,
sebbene la commit a due fasi (<xref
linkend="sql-prepare-transaction"/> e <xref
linkend="sql-commit-prepared"/>)
può essere usata per implementarlo nel codice dell'applicazione.
    </para>
   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Soluzioni commerciali</term>
   <listitem>

    <para>
Dato che <productname>PostgreSQL</productname> è open source e facilmente estendibile,  
un certo numero di compagnie hanno usato <productname>PostgreSQL</productname>
e creato soluzioni commerciali non open source con capacità di failover,
replica e bilanciamento di carico particolari.
    </para>
   </listitem>
  </varlistentry>

 </variablelist>

 <para>
<xref linkend="high-availability-matrix"/> riassume
le capacità delle varie soluzioni elencate sopra.
 </para>

 <table id="high-availability-matrix">
  <title>Matrice delle caratteristice di alta disponibilità, bilanciamento di carico e replica</title>
  <tgroup cols="8">
   <thead>
    <row>
     <entry>Feature</entry>
     <entry>Shared Disk Failover</entry>
     <entry>File System Replication</entry>
     <entry>Hot/Warm Standby Using PITR</entry>
     <entry>Trigger-Based Master-Standby Replication</entry>
     <entry>Statement-Based Replication Middleware</entry>
     <entry>Asynchronous Multimaster Replication</entry>
     <entry>Synchronous Multimaster Replication</entry>
    </row>
   </thead>

   <tbody>

    <row>
     <entry>Most Common Implementation</entry>
     <entry align="center">NAS</entry>
     <entry align="center">DRBD</entry>
     <entry align="center">PITR</entry>
     <entry align="center">Slony</entry>
     <entry align="center">pgpool-II</entry>
     <entry align="center">Bucardo</entry>
     <entry align="center"></entry>
    </row>

    <row>
     <entry>Communication Method</entry>
     <entry align="center">shared disk</entry>
     <entry align="center">disk blocks</entry>
     <entry align="center">WAL</entry>
     <entry align="center">table rows</entry>
     <entry align="center">SQL</entry>
     <entry align="center">table rows</entry>
     <entry align="center">table rows and row locks</entry>
    </row>

    <row>
     <entry>No special hardware required</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Allows multiple master servers</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>No master server overhead</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
    </row>

    <row>
     <entry>No waiting for multiple servers</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
    </row>

    <row>
     <entry>Master failure will never lose data</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Standby accept read-only queries</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">Hot only</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Per-table granularity</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>No conflict resolution necessary</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
    </row>

   </tbody>
  </tgroup>
 </table>

 <para>
Ci sono alcune soluzioni che non appartengono alle categorie sopra:
 </para>

 <variablelist>

  <varlistentry>
   <term>Partizionamento dei dati</term>
   <listitem>

    <para>
Il partizionamento dei dati divide le tabelle in insiemi di dati. Ogni insieme può 
essere modificato solamente da un server. Per esempio, i dati possono essere
partizionati per uffici, per esempio Londra e Parigi, con un server 
in ogni ufficio. Se le query che combinano i dati di  Londra e Parigi  
sono necessarie, un'applicazione può interrogare entrambi i server, o 
è possibile usare la replica master/standby per mantenere una copia 
in sola lettura dei dati dell'altro ufficio su ogni server.
    </para>
   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Esecuzione parallela di query su più server</term>
   <listitem>

    <para>
Molte delle soluzioni presentate sopra permettono la gestione di molte query da parte
di molti server, ma nessuna permette che una singola query usi molteplici server    
per terminare più velocemente. Questa soluzione permette a molti server di funzionare 
concorrentemente su una singola query. Di solito questo di ottiene 
dividendo i dati tra i server e facendo si che ogni server esegua la sua parte 
della query e restituisca i risultati al server centrale dove saranno combinati 
e restituiti all'utente. <productname>Pgpool-II</productname>
ha questa capacità. Inoltre, questo comportamento può essere implementato usando 
l'insieme di strumenti <productname>PL/Proxy</productname>.
    </para>

   </listitem>
  </varlistentry>

 </variablelist>

 </sect1>


 <sect1 id="warm-standby">
 <title>Log-Shipping Standby Servers</title>


  <para>
L'archiviazione continua può essere usate per creare un cluster ad <firstterm>alta
disponibilità</firstterm> (HA) con uno o più
<firstterm>server standby</firstterm> pronti a prendere il controllo delle operazioni se
il server primario fallisce. Ci si riferisce comunemente a questa capacità con il termine
<firstterm>warm standby</firstterm> o <firstterm>log shipping</firstterm>.
  </para>

  <para>
I server primario e standby lavorano insieme per fornire questa capacità,
sebbene i server siano accoppiati in maniera approssimativa. Il server primario opera
in modalità di archiviazione continua, mentre ogni server di standby opera
in modalità di recovery continua, leggendo i file WAL dal primario. Non è richiesto  
nessun cambiamento  alle tabelle del database per abilitare questa capacità, 
quindi offre un basso overhead di amministrazione comparato a qualche altra soluzione 
di replica. Questa configurazione ha anche un impatto relativamente basso sulle 
prestazioni del server primario.  
  </para>

  <para>
Muovere direttamente i record WAL da un database a un altro è tipicamente 
descritto come log shipping. <productname>PostgreSQL</productname>
implementa il log shipping basato sui file, che significa che i record WAL 
vengono trasferiti un file (segmento WAL) alla volta. I file WAL  (16MB) 
possono essere spediti facilmente e convenientemente a qualsiasi distanza, sia 
un sistema adiacente, che un altro sistema nello stesso sito, o un altro sistema
dall'altro lato del pianeta. La banda richiesta per questa tecnica 
varia in accordo al ritmo di transazioni del server primario. 
Il log shipping basato sui record è possibile anche tramite la replica streaming 
(si veda <xref linkend="streaming-replication"/>).
  </para>

  <para>
   It should be noted that the log shipping is asynchronous, i.e., the WAL
   records are shipped after transaction commit. As a result, there is a
   window for data loss should the primary server suffer a catastrophic
   failure; transactions not yet shipped will be lost.  The size of the
   data loss window in file-based log shipping can be limited by use of the
   <varname>archive_timeout</varname> parameter, which can be set as low
   as a few seconds.  However such a low setting will
   substantially increase the bandwidth required for file shipping.
   If you need a window of less than a minute or so, consider using
   streaming replication (see <xref linkend="streaming-replication"/>).
  </para>

  <para>
Le prestazioni per il recovery sono sufficentemente buone e quindi 
lo standby tipicamente sarà solo alcuni momenti indietro rispetto a una 
completa disponibilità una volta che è statop attivato. Come risultato, 
questa viene definita configurazione warm standby che offre alta 
disponibilità. Ripristinare un server da un backup di base archiviato e
fare un rollforward ci metterà considerevolmente di più, quindi questa tecnica 
offre solamente una soluzione per il disaster recovery, non per l'alta disponibilità.
Un server di standby può essere usato anche per query in sola lettura, nel qual caso
è chiamato server Hot Standby. Si veda <xref linkend="hot-standby"/> per maggiori 
informazioni.
  </para>

  <indexterm zone="high-availability">
   <primary>warm standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>PITR standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>standby server</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>log shipping</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>witness server</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>STONITH</primary>
  </indexterm>

  <sect2 id="standby-planning">
   <title>Planning</title>

   <para>
Di solito è consigliato creare i server primario e standby 
in modo che siano il più simili possibile, almeno dalla prospettiva
del server database. In particolare, i nomi dei percorsi associati 
ai tablespace saranno trasportati non modificati, quindi sia il server primario che  
lo standby devono avere gli stessi percorsi di mount per i 
tablespace se usano quella caratteristica. Si tenga a mente che se
<xref linkend="sql-createtablespace"/>
viene eseguita sul primario, qualsiasi  
nuovo punto di mount necessario deve essere creato sul primario e su tutti gli 
standby prima che venga eseguito il comando. L'hardware non deve essere esattamente lo stesso, 
ma l'esperienza ci porta a dire che mantenere
due sistemi identici è più facile che mantenerne due dissimili.
In ogni caso l'architettura hardware deve essere la stessa - diciamo che lo shipping
da un sistema a 32 bit ad uno a 64 bit non funzionerà.
   </para>

   <para>
In generale, il log shipping tra server che utilizzano diverse versioni maggiori di 
<productname>PostgreSQL</productname> non è possibile. La politica del 
PostgreSQL Global Development Group è di non cambiare i formati su disco 
durante aggiornamenti a versioni minori, quindi probabilmente usare diverse versioni minori 
sui server primario e standby funzionerà. Comunque, non viene fornito nessun supporto
formale per questo e si è avvisati di tenere i server primario e standby  
il più possibile allo stesso livello di versione. 
Quando si aggiorna a una nuova versione minore, la politica più sicura è di aggiornare
prima i server standby - una nuova versione minore probabilmente riuscirà 
a leggere i file WAL da una versione minore precedente piuttosto che il vice versa.
   </para>

  </sect2>

  <sect2 id="standby-server-operation">
   <title>Standby Server Operation</title>

   <para>
    In standby mode, the server continuously applies WAL received from the
    master server. The standby server can read WAL from a WAL archive
    (see <xref linkend="restore-command"/>) or directly from the master
    over a TCP connection (streaming replication). The standby server will
    also attempt to restore any WAL found in the standby cluster's
    <filename>pg_xlog</filename> directory. That typically happens after a server
    restart, when the standby replays again WAL that was streamed from the
    master before the restart, but you can also manually copy files to
    <filename>pg_xlog</filename> at any time to have them replayed.
   </para>

   <para>
    At startup, the standby begins by restoring all WAL available in the
    archive location, calling <varname>restore_command</varname>. Once it
    reaches the end of WAL available there and <varname>restore_command</varname>
    fails, it tries to restore any WAL available in the <filename>pg_xlog</filename> directory.
    If that fails, and streaming replication has been configured, the
    standby tries to connect to the primary server and start streaming WAL
    from the last valid record found in archive or <filename>pg_xlog</filename>. If that fails
    or streaming replication is not configured, or if the connection is
    later disconnected, the standby goes back to step 1 and tries to
    restore the file from the archive again. This loop of retries from the
    archive, <filename>pg_xlog</filename>, and via streaming replication goes on until the server
    is stopped or failover is triggered by a trigger file.
   </para>

   <para>
    Standby mode is exited and the server switches to normal operation,
    when a trigger file is found (<varname>trigger_file</varname>). Before failover,
    any WAL immediately available in the archive or in <filename>pg_xlog</filename> will be
    restored, but no attempt is made to connect to the master.
   </para>
  </sect2>

  <sect2 id="preparing-master-for-standby">
   <title>Preparing the Master for Standby Servers</title>

   <para>
    Set up continuous archiving on the primary to an archive directory
    accessible from the standby, as described
    in <xref linkend="continuous-archiving"/>. The archive location should be
    accessible from the standby even when the master is down, i.e. it should
    reside on the standby server itself or another trusted server, not on
    the master server.
   </para>

   <para>
    If you want to use streaming replication, set up authentication on the
    primary server to allow replication connections from the standby
    server(s); that is, provide a suitable entry or entries in
    <filename>pg_hba.conf</filename> with the database field set to
    <literal>replication</literal>.  Also ensure <varname>max_wal_senders</varname> is set
    to a sufficiently large value in the configuration file of the primary
    server.
   </para>

   <para>
    Take a base backup as described in <xref linkend="backup-base-backup"/>
    to bootstrap the standby server.
   </para>
  </sect2>

  <sect2 id="standby-server-setup">
   <title>Setting Up a Standby Server</title>

   <para>
    To set up the standby server, restore the base backup taken from primary
    server (see <xref linkend="backup-pitr-recovery"/>). Create a recovery
    command file <filename>recovery.conf</filename> in the standby's cluster data
    directory, and turn on <varname>standby_mode</varname>. Set
    <varname>restore_command</varname> to a simple command to copy files from
    the WAL archive.
   </para>

   <note>
     <para>
     Do not use pg_standby or similar tools with the built-in standby mode
     described here. <varname>restore_command</varname> should return immediately
     if the file does not exist; the server will retry the command again if
     necessary. See <xref linkend="log-shipping-alternative"/>
     for using tools like pg_standby.
    </para>
   </note>

   <para>
     If you want to use streaming replication, fill in
     <varname>primary_conninfo</varname> with a libpq connection string, including
     the host name (or IP address) and any additional details needed to
     connect to the primary server. If the primary needs a password for
     authentication, the password needs to be specified in
     <varname>primary_conninfo</varname> as well.
   </para>

   <para>
È possibile usare <varname>archive_cleanup_command</varname> per eliminare l'archivio di file
non più necessari allo standby.
   </para>

   <para>
Se si sta configurando il server standby per scopi di alta affidabilità,
impostare l'archiviazione WAL, le connessioni e l'autenticazione come sul server,
perchè il server standby funzionerà come server primario dopo il failover. 
Si dovrà anche impostare <varname>trigger_file</varname> per rendere possibile 
il failover.
Se si sta impostando il server standby per scopi di reportistica,
senza piani per il failover, allora <varname>trigger_file</varname>
non è necessario.
   </para>

   <para>
Un esempio semplice di una <filename>recovery.conf</filename> è:
<programlisting>
standby_mode = 'on'
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
restore_command = 'cp /path/to/archive/%f %p'
trigger_file = '/path/to/trigger_file'
</programlisting>
   </para>

   <para>
È possibile avere qualsiasi numero di server standby, ma se si usa la replica streaming, 
assicurarsi di impostare <varname>max_wal_senders</varname> ad un valore abbastanza alto 
nel primario da permettergli di essere connessi simultaneamente.
   </para>

   <para>
Se si sta usando un archivio WAL, la sua dimensione può essere minimizzara usando 
l'opzione <varname>archive_cleanup_command</varname> per rimuovere i file che non 
sono più richiesti dal server standby. Notare comunque, che se si sta uasndo 
l'archivio per scopi di backup, si dovranno mantenere i file necessari
per ripristinare da almeno l'ultimo backup di base, anche se non sono più necessari 
allo standby.
   </para>
  </sect2>

  <sect2 id="streaming-replication">
   <title>Replica streaming</title>

   <indexterm zone="high-availability">
    <primary>Replica streaming</primary>
   </indexterm>

   <para>
La replica streaming permette ad un server standby di rimanere più aggiornato
rispetto al log shipping basato su file. Lo standby si connette
al primario, che manda i record WAL allo standby appena vengono generati, 
senza aspettare che il file WAL sia riempito.
   </para>

   <para>
La replica streaming è asincrona, quindi c'è ancora un piccolo ritardo
tra il commit di una transazione nel primario e perchè i cambiamenti diventino 
visibili nello standby. Il ritardo comunque è molto minore rispetto 
al log shipping basato su file, tipicamente sotto il secondo, assumendo che lo standby 
sia potente abbastanza da sopportare il carico. Con la replica streaming,  
<varname>archive_timeout</varname> non è richiesto per ridurre la finestra di perdita di dati.
    window.
   </para>

   <para>
Se si usa la replica streaming senza l'archiviazione continua basata su file, 
si deve impostare <varname>wal_keep_segments</varname> nel master 
ad un valore alto abbastanza da assicurare che i segmenti WAL vecchi non vengano 
riciclati troppo presto, mentre lo standby potrebbe comunque averne bisogno. Se 
lo standby rimane troppo indietro, ha bisogno di essere inizializzato di nuovo da un
nuovo backup di base. Se si prepara un archivio WAL che sia accessibile dallo standby, 
<varname>wal_keep_segments</varname> non è richiesto dato che lo standby può sempre
usare l'archivio per aggiornarsi.
   </para>

   <para>
Per usare la replica streaming, impostare un server standby con log-shipping basato
su file come descritto in  <xref linkend="warm-standby"/>. Il passo che  
porta uno standby basato sul log shipping ad essere uno standby basato sulla
replica streaming è quello di impostare <varname>primary_conninfo</varname> nel
file <filename>recovery.conf</filename> che punti al server primario. Impostare  
<xref linkend="guc-listen-addresses"/> e le opzioni di autenticazione 
(si veda <filename>pg_hba.conf</filename>) sul primario così che il server standby 
possa connettersi allo pseudo-database <literal>replication</literal> sul server
primario (si veda <xref linkend="streaming-replication-authentication"/>).
   </para>

   <para>
Su sistemi che supportano l'opzione socket keepalive, impostare
<xref linkend="guc-tcp-keepalives-idle"/>,
<xref linkend="guc-tcp-keepalives-interval"/> e
<xref linkend="guc-tcp-keepalives-count"/> 
aiuterebbe il primario a notificare 
una connessione interrotta.
   </para>

   <para>
Impostare il numero massimo di connessioni concorrenti dai server standby
(si veda <xref linkend="guc-max-wal-senders"/> per maggiori dettagli).
   </para>

   <para>
Quando lo standby viene avviato e <varname>primary_conninfo</varname> è impostato 
correttamente, lo standby si connetterà al primario dopo aver ripetuto tutti i file WAL 
disponibile nell'archivio. Se la connessione è stabilita con successo, 
si noterà un processo walreceiver sullo standby, e un corrispondente processo
walsender sul primario.
   </para>

   <sect3 id="streaming-replication-authentication">
    <title>Autenticazione</title>
    <para>
È molto importante che i privilegi d'accesso per la replica siano impostati 
in modo che solo gli utenti fidati possano leggere lo stream WAL, dato che 
è facile estrarne informazioni privilegiate. I server standby devono autenticarsi
al primario come account superutente.
Quindi è necessario creare un ruolo con i privilegi <literal>SUPERUSER</literal> e <literal>LOGIN</literal>
sul primario.
    </para>
    <para>
L'autenticazione del client per la replica è controllata da un record nel file 
<filename>pg_hba.conf</filename> che specifica <literal>replication</literal> nel campo 
<replaceable>database</replaceable>. Per esempio, se lo standby è in esecuzione sull'IP 
<literal>192.168.1.100</literal> e il nome utente per la replica è 
<literal>foo</literal>, l'amministratore può aggiungere la linea seguente al file 
<filename>pg_hba.conf</filename> sul primario:

<programlisting>
# Permette all'utente "foo" dal host 192.168.1.100 di connettersi al primario
# come standby di replica se la password viene fornita correttamente
#
# TYPE  DATABASE        USER            CIDR-ADDRESS            METHOD
host    replication     foo             192.168.1.100/32        md5
</programlisting>
    </para>
    <para>
Il nome del host e il numero della porta del primario, il nome utente per la connessione
e la password, sono specificati nel file <filename>recovery.conf</filename>.
La password può essere impostata anche nel file <filename>~/.pgpass</filename> sullo standby
(specificare <literal>replication</literal> nel campo <replaceable>database</replaceable>).
Per esempio, se il primario è in esecuzione sull'IP <literal>192.168.1.50</literal>,
porta <literal>5432</literal>, il nome del superutente per la replica è 
<literal>foo</literal>, e la password è <literal>foopass</literal>, l'amministratore 
può aggiungere la seguente linea al file <filename>recovery.conf</filename> sullo standby:

<programlisting>
# The standby connects to the primary that is running on host 192.168.1.50
# and port 5432 as the user "foo" whose password is "foopass".
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
</programlisting>
    </para>
   </sect3>

   <sect3 id="streaming-replication-monitoring">
    <title>Monitoraggio</title>
    <para>
Un indicatore di salute della replica streaming importante è la quantità
di record WAL generati nel primario, ma non ancora applicati nello stanby.
È possibile calcolare questo ritardo confrontando la locazione di scrittura 
dello WAL corrente sul primario e l'ultima locazione ricevuta dallo standby.
Esse possono essere ottenute usando <function>pg_current_xlog_location</function>
sul primario e <function>pg_last_xlog_receive_location</function>
sullo standby, rispettivamente (si veda <xref linkend="functions-admin-backup-table"/> e 
<xref linkend="functions-recovery-info-table"/> per maggiori dettagli).
L'ultima locazione WAL ricevuta nello standby viene visualizzata anche nello stato
del processo WAL receiver, visibile con il comando 
<command>ps</command> (si veda <xref linkend="monitoring-ps"/> per maggiori dettagli).
    </para>
   </sect3>

  </sect2>
  </sect1>

  <sect1 id="warm-standby-failover">
   <title>Failover</title>

   <para>
Se il server primario fallisce allora il server standby dovrebbe iniziare le
procedure di failover. 
   </para>

   <para>
Se il server standby fallisce allora non c'è bisogno che avvenga alcun failover. Se 
il server standby può essere riavviato, anche più tardi, allora anche il processo di
recovery può essere riavviato immediatamente, ottenendo il vantaggio del recovery 
riavviabile. Se il server standby non può essere riavviato, allora dovrebbe essere
creata un'istanza completamente nuova del server standby. 
   </para>

   <para>
Se il server primario fallisce e il server standby diventa il nuovo primario,  
e poi il vecchio primario viene riavviato, si deve avere un
meccanismo per informare il vecchio primario che non è più il primario. A volte ci si 
riferisce a quasto come <acronym>STONITH</acronym> (Shoot The Other Node In The Head), che 
è per evitare situazioni dove entrambi i sistemi pensano di essere il primario, 
che genera confusione ed essenzialmente perdita di dati.
   </para>

   <para>
    Many failover systems use just two systems, the primary and the standby,
    connected by some kind of heartbeat mechanism to continually verify the
    connectivity between the two and the viability of the primary. It is
    also possible to use a third system (called a witness server) to prevent
    some cases of inappropriate failover, but the additional complexity
    might not be worthwhile unless it is set up with sufficient care and
    rigorous testing.
   </para>

   <para>
<productname>PostgreSQL</productname> non fornisce il software di sistema necessario 
a identificare un fallimento sul primario e notificare il server database di standby.
Molti di questi strumenti esistono e sono ben integrati 
con le strutture del sistema operativo richieste per un failover 
riuscito bene, tipo la migrazione dell'indirizzo IP.
   </para>

   <para>
Una volta che avviene il failover verso lo standby, c'è solo un 
singolo server in funzione. Questo è noto essere uno stato corrotto.  
Lo standby precedente adesso è il primario, ma il rpecedente primario è giu e potrebbe 
rimanere giù. Per tornare alle operazioni normali, deve essere ricreato
un server standby,
o sul precedente sistema primario quando viene tirato su, o su un terzo sistema,
possibilmente nuovo. Una volta completato possiamo considerare invertiti
i ruoli di primario e standby. Alcune persone scelgono di usare un terzo server 
per avere dei backup del nuovo primario finchè il nuovo server standby non è ricreato,  
sebbene questo chiaramente complichi le configurazioni dei sistemi 
e i processi operazionali.
   </para>

   <para>
Quindi, passare da primario a standby può essere veloce ma richiede
un po' di tempo per ripreparare il cluster di failover. Il passaggio regolare  
da primario a standby è utile, dato che permette periodi di fermo regolari di ogni sistema
per manutenzione. Questo serve anche come test del meccanismo di failoevr
per assicurare che funzionerà realmente quando se ne avrà bisogno. 
Sono consigliate procedure di amministrazione scritte. 
   </para>

   <para>
    To trigger failover of a log-shipping standby server, create a trigger
    file with the filename and path specified by the <varname>trigger_file</varname>
    setting in <filename>recovery.conf</filename>. If <varname>trigger_file</varname> is
    not given, there is no way to exit recovery in the standby and promote
    it to a master. That can be useful for e.g reporting servers that are
    only used to offload read-only queries from the primary, not for high
    availability purposes.
   </para>
  </sect1>

  <sect1 id="log-shipping-alternative">
   <title>Metodo alternativo per il log shipping</title>

   <para>
Un'alternativa alla modalità standby incorporata descritta nelle precedenti 
sezioni è di usare un <varname>restore_command</varname> che sonda la posizione dell'archivio.
Questa era la sola opzione disponibile nelle versioni 8.4 e precedenti. In questa 
configurazione, impostare <varname>standby_mode</varname> a off, dato che si sta implementando
da soli il polling richiesto per l'operazione di standby. Si veda
contrib/pg_standby (<xref linkend="pgstandby"/>) per un'implementazione di riferimento
di questo.
   </para>

   <para>
Si noti che in questa modalità, il server applicherà lo WAL un file 
alla volta, quindi se si usa il server standby per query (si veda Hot Standby),
c'è un ritardo tra un'azione nel master e quando l'azione 
diventa visibile nello standby, corrispondendo al tempo che ci mette a riempire
il file WAL. <varname>archive_timeout</varname> può essere usato per rendere minore il ritardo. 
Notare inoltre che non è possibile combinare la replica streaming con 
questo metodo.
   </para>

   <para>
Le operazioni che avvengono sia sul primario che sullo standby sono 
normali compiti di archiviazione continua e ripristino. Il solo punto di  
contatto tra i due server database è l'archivio di file WAL
che entrambi condividono: il primario scrivendo l'archivio, lo standby leggendo
dall'archivio. Dev'essere posta attenzione per essere sicuri che gli archivi WAL 
da server primari separati non si mescolino tra loro. L'archivio non deve 
essere grande se è richiesto solo per operazioni di standby.
   </para>

   <para>
Il trucco che fa funzionare insieme i due server è semplicemente un 
<varname>restore_command</varname> usato sullo standby che, 
quando richiesto per il prossimo file WAL, aspetta che diventi disponibile dal primario.
Il <varname>restore_command</varname> viene specificato nel file 
<filename>recovery.conf</filename> sul server stanby. Il processo di  ripristino 
normale richiederebbe un file dall'archivio WAL, riportando un fallimento 
se il file non era disponibile. Per il processamento dello standby è normale 
che il prossimo file WAL non sia disponibile, quindi lo standby deve aspettare 
che appaia. Per file che terminano con <literal>.backup</literal> o 
<literal>.history</literal> non c'è bisogno di aspettare, e un deve essere restituito 
un codice diverso da zero. 
Un <varname>restore_command</varname> di attesa può essere 
scritto come script personalizzato che cicla dopo aver fatto il pool per l'esistenza del 
prossimo file WAL. Ci deve essere anche qualche modo di triggerare il failover, che
dovrebbe interrompere il <varname>restore_command</varname>, rompere il ciclo e
ritornare un errore di tipo file-non-trovato al server standby. Questo termina il ripristino
e lo standby quindi tornerà attivo come un server normale.
   </para>

   <para>
    Pseudocode for a suitable <varname>restore_command</varname> is:
<programlisting>
triggered = false;
while (!NextWALFileReady() &amp;&amp; !triggered)
{
    sleep(100000L);         /* wait for ~0.1 sec */
    if (CheckForExternalTrigger())
        triggered = true;
}
if (!triggered)
        CopyWALFileForRecovery();
</programlisting>
   </para>

   <para>
    A working example of a waiting <varname>restore_command</varname> is provided
    as a <filename>contrib</filename> module named <application>pg_standby</application>. It
    should be used as a reference on how to correctly implement the logic
    described above. It can also be extended as needed to support specific
    configurations and environments.
   </para>

   <para>
    The method for triggering failover is an important part of planning
    and design. One potential option is the <varname>restore_command</varname>
    command.  It is executed once for each WAL file, but the process
    running the <varname>restore_command</varname> is created and dies for
    each file, so there is no daemon or server process, and
    signals or a signal handler cannot be used. Therefore, the
    <varname>restore_command</varname> is not suitable to trigger failover.
    It is possible to use a simple timeout facility, especially if
    used in conjunction with a known <varname>archive_timeout</varname>
    setting on the primary. However, this is somewhat error prone
    since a network problem or busy primary server might be sufficient
    to initiate failover. A notification mechanism such as the explicit
    creation of a trigger file is ideal, if this can be arranged.
   </para>

  <sect2 id="warm-standby-config">
   <title>Implementation</title>

   <para>
    The short procedure for configuring a standby server using this alternative
    method is as follows. For
    full details of each step, refer to previous sections as noted.
    <orderedlist>
     <listitem>
      <para>
       Set up primary and standby systems as nearly identical as
       possible, including two identical copies of
       <productname>PostgreSQL</productname> at the same release level.
      </para>
     </listitem>
     <listitem>
      <para>
       Set up continuous archiving from the primary to a WAL archive
       directory on the standby server. Ensure that
       <xref linkend="guc-archive-mode"/>,
       <xref linkend="guc-archive-command"/> and
       <xref linkend="guc-archive-timeout"/>
       are set appropriately on the primary
       (see <xref linkend="backup-archiving-wal"/>).
      </para>
     </listitem>
     <listitem>
      <para>
       Make a base backup of the primary server (see <xref
       linkend="backup-base-backup"/>), and load this data onto the standby.
      </para>
     </listitem>
     <listitem>
      <para>
Cominciare il ripristino sul server standby dall'archivio WAL locale,
usare un <filename>recovery.conf</filename> che specifica un 
 <varname>restore_command</varname> che aspetta come descritto 
precedentemente (si veda <xref linkend="backup-pitr-recovery"/>).
      </para>
     </listitem>
    </orderedlist>
   </para>

   <para>
    Recovery treats the WAL archive as read-only, so once a WAL file has
Il rispristino tratta l'archivio WAL in sola lettura, quindi una volta che una
    been copied to the standby system it can be copied to tape at the same

    time as it is being read by the standby database server.
    Thus, running a standby server for high availability can be performed at
    the same time as files are stored for longer term disaster recovery
    purposes.
   </para>

   <para>
    For testing purposes, it is possible to run both primary and standby
    servers on the same system. This does not provide any worthwhile
    improvement in server robustness, nor would it be described as HA.
   </para>
  </sect2>

  <sect2 id="warm-standby-record">
   <title>Record-based Log Shipping</title>

   <para>
    It is also possible to implement record-based log shipping using this
    alternative method, though this requires custom development, and changes
    will still only become visible to hot standby queries after a full WAL
    file has been shipped.
   </para>

   <para>
    An external program can call the <function>pg_xlogfile_name_offset()</function>
    function (see <xref linkend="functions-admin"/>)
    to find out the file name and the exact byte offset within it of
    the current end of WAL.  It can then access the WAL file directly
    and copy the data from the last known end of WAL through the current end
    over to the standby servers.  With this approach, the window for data
    loss is the polling cycle time of the copying program, which can be very
    small, and there is no wasted bandwidth from forcing partially-used
    segment files to be archived.  Note that the standby servers'
    <varname>restore_command</varname> scripts can only deal with whole WAL files,
    so the incrementally copied data is not ordinarily made available to
    the standby servers.  It is of use only when the primary dies &mdash;
    then the last partial WAL file is fed to the standby before allowing
    it to come up.  The correct implementation of this process requires
    cooperation of the <varname>restore_command</varname> script with the data
    copying program.
   </para>

   <para>
    Starting with <productname>PostgreSQL</productname> version 9.0, you can use
    streaming replication (see <xref linkend="streaming-replication"/>) to
    achieve the same benefits with less effort.
   </para>
  </sect2>
 </sect1>

 <sect1 id="hot-standby">
  <title>Hot Standby</title>

  <indexterm zone="high-availability">
   <primary>Hot Standby</primary>
  </indexterm>

   <para>
    Hot Standby is the term used to describe the ability to connect to
    the server and run read-only queries while the server is in archive
    recovery or standby mode. This
    is useful both for replication purposes and for restoring a backup
    to a desired state with great precision.
    The term Hot Standby also refers to the ability of the server to move
    from recovery through to normal operation while users continue running
    queries and/or keep their connections open.
   </para>

   <para>
    Running queries in hot standby mode is similar to normal query operation,
    though there are several usage and administrative differences
    explained below.
   </para>

  <sect2 id="hot-standby-users">
   <title>User's Overview</title>

   <para>
    When the <xref linkend="guc-hot-standby"/> parameter is set to true on a
    standby server, it will begin accepting connections once the recovery has
    brought the system to a consistent state.  All such connections are
    strictly read-only; not even temporary tables may be written.
   </para>

   <para>
    The data on the standby takes some time to arrive from the primary server
    so there will be a measurable delay between primary and standby. Running the
    same query nearly simultaneously on both primary and standby might therefore
    return differing results. We say that data on the standby is
    <firstterm>eventually consistent</firstterm> with the primary.  Once the
    commit record for a transaction is replayed on the standby, the changes
    made by that transaction will be visible to any new snapshots taken on
    the standby.  Snapshots may be taken at the start of each query or at the
    start of each transaction, depending on the current transaction isolation
    level.  For more details, see <xref linkend="transaction-iso"/>.
   </para>

   <para>
    Transactions started during hot standby may issue the following commands:

    <itemizedlist>
     <listitem>
      <para>
       Query access - <command>SELECT</command>, <command>COPY TO</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Cursor commands - <command>DECLARE</command>, <command>FETCH</command>, <command>CLOSE</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Parameters - <command>SHOW</command>, <command>SET</command>, <command>RESET</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Transaction management commands
        <itemizedlist>
         <listitem>
          <para>
           <command>BEgin</command>, <command>END</command>, <command>ABORT</command>, <command>START TRANSACTION</command>
          </para>
         </listitem>
         <listitem>
          <para>
           <command>SAVEPOINT</command>, <command>RELEASE</command>, <command>ROLLBACK TO SAVEPOINT</command>
          </para>
         </listitem>
         <listitem>
          <para>
           <command>EXCEPTION</command> blocks and other internal subtransactions
          </para>
         </listitem>
        </itemizedlist>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK TABLE</command>, though only when explicitly in one of these modes:
       <literal>ACCESS SHARE</literal>, <literal>ROW SHARE</literal> or <literal>ROW EXCLUSIVE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Plans and resources - <command>PREPARE</command>, <command>EXECUTE</command>,
       <command>DEALLOCATE</command>, <command>DISCARD</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Plugins and extensions - <command>LOAD</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Transactions started during hot standby will never be assigned a
    transaction ID and cannot write to the system write-ahead log.
    Therefore, the following actions will produce error messages:

    <itemizedlist>
     <listitem>
      <para>
       Data Manipulation Language (DML) - <command>INSERT</command>,
       <command>UPDATE</command>, <command>DELETE</command>, <command>COPY FROM</command>,
       <command>TRUNCATE</command>.
       Note that there are no allowed actions that result in a trigger
       being executed during recovery.  This restriction applies even to
       temporary tables, because table rows cannot be read or written without
       assigning a transaction ID, which is currently not possible in a
       Hot Standby environment.
      </para>
     </listitem>
     <listitem>
      <para>
       Data Definition Language (DDL) - <command>CREATE</command>,
       <command>DROP</command>, <command>ALTER</command>, <command>COMMENT</command>.
       This restriction applies even to temporary tables, because carrying
       out these operations would require updating the system catalog tables.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>SELECT ... FOR SHARE | UPDATE</command>, because row locks cannot be
       taken without updating the underlying data files.
      </para>
     </listitem>
     <listitem>
      <para>
       Rules on <command>SELECT</command> statements that generate DML commands.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK</command> that explicitly requests a mode higher than <literal>ROW EXCLUSIVE MODE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK</command> in short default form, since it requests <literal>ACCESS EXCLUSIVE MODE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Transaction management commands that explicitly set non-read-only state:
        <itemizedlist>
         <listitem>
          <para>
            <command>BEgin READ WRITE</command>,
            <command>START TRANSACTION READ WRITE</command>
          </para>
         </listitem>
         <listitem>
          <para>
            <command>SET TRANSACTION READ WRITE</command>,
            <command>SET SESSION CHARACTERISTICS AS TRANSACTION READ WRITE</command>
          </para>
         </listitem>
         <listitem>
          <para>
           <command>SET transaction_read_only = off</command>
          </para>
         </listitem>
        </itemizedlist>
      </para>
     </listitem>
     <listitem>
      <para>
       Two-phase commit commands - <command>PREPARE TRANSACTION</command>,
       <command>COMMIT PREPARED</command>, <command>ROLLBACK PREPARED</command>
       because even read-only transactions need to write WAL in the
       prepare phase (the first phase of two phase commit).
      </para>
     </listitem>
     <listitem>
      <para>
       Sequence updates - <function>nextval()</function>, <function>setval()</function>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LISTEN</command>, <command>UNLISTEN</command>, <command>NOTIFY</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    In normal operation, <quote>read-only</quote> transactions are allowed to
    update sequences and to use <command>LISTEN</command>, <command>UNLISTEN</command>, and
    <command>NOTIFY</command>, so Hot Standby sessions operate under slightly tighter
    restrictions than ordinary read-only sessions.  It is possible that some
    of these restrictions might be loosened in a future release.
   </para>

   <para>
    During hot standby, the parameter <varname>transaction_read_only</varname> is always
    true and may not be changed.  But as long as no attempt is made to modify
    the database, connections during hot standby will act much like any other
    database connection.  If failover or switchover occurs, the database will
    switch to normal processing mode.  Sessions will remain connected while the
    server changes mode.  Once hot standby finishes, it will be possible to
    initiate read-write transactions (even from a session begun during
    hot standby).
   </para>

   <para>
    Users will be able to tell whether their session is read-only by
    issuing <command>SHOW transaction_read_only</command>.  In addition, a set of
    functions (<xref linkend="functions-recovery-info-table"/>) allow users to
    access information about the standby server. These allow you to write
    programs that are aware of the current state of the database. These
    can be used to monitor the progress of recovery, or to allow you to
    write complex programs that restore the database to particular states.
   </para>
  </sect2>

  <sect2 id="hot-standby-conflict">
   <title>Handling query conflicts</title>

   <para>
    The primary and standby servers are in many ways loosely connected. Actions
    on the primary will have an effect on the standby. As a result, there is
    potential for negative interactions or conflicts between them. The easiest
    conflict to understand is performance: if a huge data load is taking place
    on the primary then this will generate a similar stream of WAL records on the
    standby, so standby queries may contend for system resources, such as I/O.
   </para>

   <para>
    There are also additional types of conflict that can occur with Hot Standby.
    These conflicts are <emphasis>hard conflicts</emphasis> in the sense that queries
    might need to be cancelled and, in some cases, sessions disconnected to resolve them.
    The user is provided with several ways to handle these
    conflicts. Conflict cases include:

      <itemizedlist>
       <listitem>
        <para>
         Access Exclusive locks taken on the primary server, including both
         explicit <command>LOCK</command> commands and various <acronym>DDL</acronym>
         actions, conflict with table accesses in standby queries.
        </para>
       </listitem>
       <listitem>
        <para>
         Dropping a tablespace on the primary conflicts with standby queries
         using that tablespace for temporary work files.
        </para>
       </listitem>
       <listitem>
        <para>
         Dropping a database on the primary conflicts with sessions connected
         to that database on the standby.
        </para>
       </listitem>
       <listitem>
        <para>
         Application of a vacuum cleanup record from WAL conflicts with
         standby transactions whose snapshots can still <quote>see</quote> any of
         the rows to be removed.
        </para>
       </listitem>
       <listitem>
        <para>
         Application of a vacuum cleanup record from WAL conflicts with
         queries accessing the target page on the standby, whether or not
         the data to be removed is visible.
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    On the primary server, these cases simply result in waiting; and the
    user might choose to cancel either of the conflicting actions.  However,
    on the standby there is no choice: the WAL-logged action already occurred
    on the primary so the standby must not fail to apply it.  Furthermore,
    allowing WAL application to wait indefinitely may be very undesirable,
    because the standby's state will become increasingly far behind the
    primary's.  Therefore, a mechanism is provided to forcibly cancel standby
    queries that conflict with to-be-applied WAL records.
   </para>

   <para>
    An example of the problem situation is an administrator on the primary
    server running <command>DROP TABLE</command> on a table that is currently being
    queried on the standby server.  Clearly the standby query cannot continue
    if the <command>DROP TABLE</command> is applied on the standby. If this situation
    occurred on the primary, the <command>DROP TABLE</command> would wait until the
    other query had finished. But when <command>DROP TABLE</command> is run on the
    primary, the primary doesn't have information about what queries are
    running on the standby, so it will not wait for any such standby
    queries. The WAL change records come through to the standby while the
    standby query is still running, causing a conflict.  The standby server
    must either delay application of the WAL records (and everything after
    them, too) or else cancel the conflicting query so that the <command>DROP
    TABLE</command> can be applied.
   </para>

   <para>
    When a conflicting query is short, it's typically desirable to allow it to
    complete by delaying WAL application for a little bit; but a long delay in
    WAL application is usually not desirable.  So the cancel mechanism has
    parameters, <xref linkend="guc-max-standby-archive-delay"/> and <xref
    linkend="guc-max-standby-streaming-delay"/>, that define the maximum
    allowed delay in WAL application.  Conflicting queries will be canceled
    once it has taken longer than the relevant delay setting to apply any
    newly-received WAL data.  There are two parameters so that different delay
    values can be specified for the case of reading WAL data from an archive
    (i.e., initial recovery from a base backup or <quote>catching up</quote> a
    standby server that has fallen far behind) versus reading WAL data via
    streaming replication.
   </para>

   <para>
    In a standby server that exists primarily for high availability, it's
    best to set the delay parameters relatively short, so that the server
    cannot fall far behind the primary due to delays caused by standby
    queries.  However, if the standby server is meant for executing
    long-running queries, then a high or even infinite delay value may be
    preferable.  Keep in mind however that a long-running query could
    cause other sessions on the standby server to not see recent changes
    on the primary, if it delays application of WAL records.
   </para>

   <para>
    The most common reason for conflict between standby queries and WAL replay
    is <quote>early cleanup</quote>.  Normally, <productname>PostgreSQL</productname> allows
    cleanup of old row versions when there are no transactions that need to
    see them to ensure correct visibility of data according to MVCC rules.
    However, this rule can only be applied for transactions executing on the
    master.  So it is possible that cleanup on the master will remove row
    versions that are still visible to a transaction on the standby.
   </para>

   <para>
    Experienced users should note that both row version cleanup and row version
    freezing will potentially conflict with standby queries. Running a manual
    <command>VACUUM FREEZE</command> is likely to cause conflicts even on tables with
    no updated or deleted rows.
   </para>

   <para>
    Once the delay specified by <varname>max_standby_archive_delay</varname> or
    <varname>max_standby_streaming_delay</varname> has been exceeded, conflicting
    queries will be cancelled.  This usually results just in a cancellation
    error, although in the case of replaying a <command>DROP DATABASE</command>
    the entire conflicting session will be terminated.  Also, if the conflict
    is over a lock held by an idle transaction, the conflicting session is
    terminated (this behavior might change in the future).
   </para>

   <para>
    Cancelled queries may be retried immediately (after beginning a new
    transaction, of course).  Since query cancellation depends on
    the nature of the WAL records being replayed, a query that was
    cancelled may well succeed if it is executed again.
   </para>

   <para>
    Keep in mind that the delay parameters are compared to the elapsed time
    since the WAL data was received by the standby server.  Thus, the grace
    period allowed to any one query on the standby is never more than the
    delay parameter, and could be considerably less if the standby has already
    fallen behind as a result of waiting for previous queries to complete, or
    as a result of being unable to keep up with a heavy update load.
   </para>

   <para>
    Users should be clear that tables that are regularly and heavily updated
    on the primary server will quickly cause cancellation of longer running
    queries on the standby. In such cases the setting of a finite value for
    <varname>max_standby_archive_delay</varname> or
    <varname>max_standby_streaming_delay</varname> can be considered similar to
    setting <varname>statement_timeout</varname>.
   </para>

   <para>
    Remedial possibilities exist if the number of standby-query cancellations
    is found to be unacceptable.  The first option is to connect to the
    primary server and keep a query active for as long as needed to
    run queries on the standby. This prevents <command>VACUUM</command> from removing
    recently-dead rows and so cleanup conflicts do not occur.
    This could be done using <filename>contrib/dblink</filename> and
    <function>pg_sleep()</function>, or via other mechanisms. If you do this, you
    should note that this will delay cleanup of dead rows on the primary,
    which may result in undesirable table bloat. However, the cleanup
    situation will be no worse than if the standby queries were running
    directly on the primary server, and you are still getting the benefit of
    off-loading execution onto the standby.
    <varname>max_standby_archive_delay</varname> must be kept large in this case,
    because delayed WAL files might already contain entries that conflict with
    the desired standby queries.
   </para>

   <para>
    Another option is to increase <xref linkend="guc-vacuum-defer-cleanup-age"/>
    on the primary server, so that dead rows will not be cleaned up as quickly
    as they normally would be.  This will allow more time for queries to
    execute before they are cancelled on the standby, without having to set
    a high <varname>max_standby_streaming_delay</varname>.  However it is
    difficult to guarantee any specific execution-time window with this
    approach, since <varname>vacuum_defer_cleanup_age</varname> is measured in
    transactions executed on the primary server.
   </para>
  </sect2>

  <sect2 id="hot-standby-admin">
   <title>Administrator's Overview</title>

   <para>
    If <varname>hot_standby</varname> is turned <literal>on</literal> in
Se  <varname>hot_standby</varname> è <literal>on</literal> in   
    <filename>postgresql.conf</filename> and there is a <filename>recovery.conf</filename>
nel <filename>postgresql.conf</filename> ed è presente un file <filename>recovery.conf</filename>,
    file present, the server will run in Hot Standby mode.
 
    However, it may take some time for Hot Standby connections to be allowed,
    because the server will not accept connections until it has completed
    sufficient recovery to provide a consistent state against which queries
    can run.  During this period,
    clients that attempt to connect will be refused with an error message.
    To confirm the server has come up, either loop trying to connect from
    the application, or look for these messages in the server logs:

<programlisting>
LOG:  entering standby mode

... then some time later ...

LOG:  consistent recovery state reached
LOG:  database system is ready to accept read only connections
</programlisting>

    Consistency information is recorded once per checkpoint on the primary.
    It is not possible to enable hot standby when reading WAL
    written during a period when <varname>wal_level</varname> was not set to
    <literal>hot_standby</literal> on the primary.  Reaching a consistent state can
    also be delayed in the presence of both of these conditions:

      <itemizedlist>
       <listitem>
        <para>
         A write transaction has more than 64 subtransactions
        </para>
       </listitem>
       <listitem>
        <para>
         Very long-lived write transactions
        </para>
       </listitem>
      </itemizedlist>

    If you are running file-based log shipping ("warm standby"), you might need
    to wait until the next WAL file arrives, which could be as long as the
    <varname>archive_timeout</varname> setting on the primary.
   </para>

   <para>
    The setting of some parameters on the standby will need reconfiguration
    if they have been changed on the primary. For these parameters,
    the value on the standby must
    be equal to or greater than the value on the primary. If these parameters
    are not set high enough then the standby will refuse to start.
    Higher values can then be supplied and the server
    restarted to begin recovery again.  These parameters are:

      <itemizedlist>
       <listitem>
        <para>
         <varname>max_connections</varname>
        </para>
       </listitem>
       <listitem>
        <para>
         <varname>max_prepared_transactions</varname>
        </para>
       </listitem>
       <listitem>
        <para>
         <varname>max_locks_per_transaction</varname>
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    It is important that the administrator select appropriate settings for
    <xref linkend="guc-max-standby-archive-delay"/> and <xref
    linkend="guc-max-standby-streaming-delay"/>.  The best choices vary
    depending on business priorities.  For example if the server is primarily
    tasked as a High Availability server, then you will want low delay
    settings, perhaps even zero, though that is a very aggressive setting. If
    the standby server is tasked as an additional server for decision support
    queries then it might be acceptable to set the maximum delay values to
    many hours, or even -1 which means wait forever for queries to complete.
   </para>

   <para>
    Transaction status "hint bits" written on the primary are not WAL-logged,
    so data on the standby will likely re-write the hints again on the standby.
    Thus, the standby server will still perform disk writes even though
    all users are read-only; no changes occur to the data values
    themselves.  Users will still write large sort temporary files and
    re-generate relcache info files, so no part of the database
    is truly read-only during hot standby mode.
    Note also that writes to remote databases using
    <application>dblink</application> module, and other operations outside the
    database using PL functions will still be possible, even though the
    transaction is read-only locally.
   </para>

   <para>
    The following types of administration commands are not accepted
    during recovery mode:

      <itemizedlist>
       <listitem>
        <para>
         Data Definition Language (DDL) - e.g. <command>CREATE INDEX</command>
        </para>
       </listitem>
       <listitem>
        <para>
         Privilege and Ownership - <command>GRANT</command>, <command>REVOKE</command>,
         <command>REASSIGN</command>
        </para>
       </listitem>
       <listitem>
        <para>
         Maintenance commands - <command>ANALYZE</command>, <command>VACUUM</command>,
         <command>CLUSTER</command>, <command>REINDEX</command>
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    Again, note that some of these commands are actually allowed during
    "read only" mode transactions on the primary.
   </para>

   <para>
    As a result, you cannot create additional indexes that exist solely
    on the standby, nor statistics that exist solely on the standby.
    If these administration commands are needed, they should be executed
    on the primary, and eventually those changes will propagate to the
    standby.
   </para>

   <para>
    <function>pg_cancel_backend()</function> will work on user backends, but not the
    Startup process, which performs recovery. <structname>pg_stat_activity</structname> does not
    show an entry for the Startup process, nor do recovering transactions
    show as active. As a result, <structname>pg_prepared_xacts</structname> is always empty during
    recovery. If you wish to resolve in-doubt prepared transactions,
    view <literal>pg_prepared_xacts</literal> on the primary and issue commands to
    resolve transactions there.
   </para>

   <para>
    <structname>pg_locks</structname> will show locks held by backends,
    as normal. <structname>pg_locks</structname> also shows
    a virtual transaction managed by the Startup process that owns all
    <literal>AccessExclusiveLocks</literal> held by transactions being replayed by recovery.
    Note that the Startup process does not acquire locks to
    make database changes, and thus locks other than <literal>AccessExclusiveLocks</literal>
    do not show in <structname>pg_locks</structname> for the Startup
    process; they are just presumed to exist.
   </para>

   <para>
    The <productname>Nagios</productname> plugin <productname>check_pgsql</productname> will
    work, because the simple information it checks for exists.
    The <productname>check_postgres</productname> monitoring script will also work,
    though some reported values could give different or confusing results.
    For example, last vacuum time will not be maintained, since no
    vacuum occurs on the standby.  Vacuums running on the primary
    do still send their changes to the standby.
   </para>

   <para>
    WAL file control commands will not work during recovery,
    e.g. <function>pg_start_backup</function>, <function>pg_switch_xlog</function> etc.
   </para>

   <para>
    Dynamically loadable modules work, including <structname>pg_stat_statements</structname>.
   </para>

   <para>
    Advisory locks work normally in recovery, including deadlock detection.
    Note that advisory locks are never WAL logged, so it is impossible for
    an advisory lock on either the primary or the standby to conflict with WAL
    replay. Nor is it possible to acquire an advisory lock on the primary
    and have it initiate a similar advisory lock on the standby. Advisory
    locks relate only to the server on which they are acquired.
   </para>

   <para>
    Trigger-based replication systems such as <productname>Slony</productname>,
    <productname>Londiste</productname> and <productname>Bucardo</productname> won't run on the
    standby at all, though they will run happily on the primary server as
    long as the changes are not sent to standby servers to be applied.
    WAL replay is not trigger-based so you cannot relay from the
    standby to any system that requires additional database writes or
    relies on the use of triggers.
   </para>

   <para>
    New OIDs cannot be assigned, though some <acronym>UUID</acronym> generators may still
    work as long as they do not rely on writing new status to the database.
   </para>

   <para>
    Currently, temporary table creation is not allowed during read only
    transactions, so in some cases existing scripts will not run correctly.
    This restriction might be relaxed in a later release. This is
    both a SQL Standard compliance issue and a technical issue.
   </para>

   <para>
    <command>DROP TABLESPACE</command> can only succeed if the tablespace is empty.
    Some standby users may be actively using the tablespace via their
    <varname>temp_tablespaces</varname> parameter. If there are temporary files in the
    tablespace, all active queries are cancelled to ensure that temporary
    files are removed, so the tablespace can be removed and WAL replay
    can continue.
   </para>

   <para>
    Running <command>DROP DATABASE</command>, <command>ALTER DATABASE ... SET
    TABLESPACE</command>, or <command>ALTER DATABASE ... RENAME</command> on the primary
    will generate a WAL entry that will cause all users connected to that
    database on the standby to be forcibly disconnected. This action occurs
    immediately, whatever the setting of
    <varname>max_standby_streaming_delay</varname>.
   </para>

   <para>
    In normal (non-recovery) mode, if you issue <command>DROP USER</command> or <command>DROP ROLE</command>
    for a role with login capability while that user is still connected then
    nothing happens to the connected user - they remain connected. The user cannot
    reconnect however. This behavior applies in recovery also, so a
    <command>DROP USER</command> on the primary does not disconnect that user on the standby.
   </para>

   <para>
    The statistics collector is active during recovery. All scans, reads, blocks,
    index usage, etc., will be recorded normally on the standby. Replayed
    actions will not duplicate their effects on primary, so replaying an
    insert will not increment the Inserts column of pg_stat_user_tables.
    The stats file is deleted at the start of recovery, so stats from primary
    and standby will differ; this is considered a feature, not a bug.
   </para>

   <para>
    Autovacuum is not active during recovery.  It will start normally at the
    end of recovery.
   </para>

   <para>
    The background writer is active during recovery and will perform
    restartpoints (similar to checkpoints on the primary) and normal block
    cleaning activities. This can include updates of the hint bit
    information stored on the standby server.
    The <command>CHECKPOINT</command> command is accepted during recovery,
    though it performs a restartpoint rather than a new checkpoint.
   </para>
  </sect2>

  <sect2 id="hot-standby-parameters">
   <title>Hot Standby Parameter Reference</title>

   <para>
    Various parameters have been mentioned above in
    <xref linkend="hot-standby-conflict"/> and
    <xref linkend="hot-standby-admin"/>.
   </para>

   <para>
    On the primary, parameters <xref linkend="guc-wal-level"/> and
    <xref linkend="guc-vacuum-defer-cleanup-age"/> can be used.
    <xref linkend="guc-max-standby-archive-delay"/> and
    <xref linkend="guc-max-standby-streaming-delay"/> have no effect if set on
    the primary.
   </para>

   <para>
Sullo standby, possono essere usati i parametri <xref linkend="guc-hot-standby"/>,
<xref linkend="guc-max-standby-archive-delay"/> e
<xref linkend="guc-max-standby-streaming-delay"/>.
<xref linkend="guc-vacuum-defer-cleanup-age"/> non ha effetto
fino a che il server rimane in modalità standby, sebbene diventi rilevante
se lo standby diventa il primario. 
   </para>
  </sect2>

  <sect2 id="hot-standby-caveats">
   <title>Avvertimenti</title>

   <para>
Hot Standby ha diverse limitazioni. 
Queste potranno e probabilmente saranno corrette in versioni future:

  <itemizedlist>
   <listitem>
    <para>
Le operazioni sugli indici hash attualmente non sono sottoposte a log nel WAL, 
quindi rifarle non aggiornerà questi indici. Gli indici hash non verrano 
usati per i piani delle query durante il ripristino.
    </para>
   </listitem>
   <listitem>
    <para>
È richiesta la conoscienza completa delle transazioni in esecuzione prima che possano
essere realizzati degli snapshot. Transazioni che usano grandi numeri di transazioni
(attualmente maggiori di 64) ritarderanno l'inizio delle connessioni in sola lettura  
fino al completamento della transazione in esecuzione più lunga. 
Se si presenta questa situazione, verrano spediti al log messaggi di spiegazione. 
    </para>
   </listitem>
   <listitem>
    <para>
Punti d'inizio validi per query standby vengono generati ad ogni checkpoint 
sul master. Se lo standby viene spento mentre il master è in uno stato di shutdown,
potrebbe non essere possibile di rientrare in Hot Standby 
fino a che il primario viene fatto partire, così che generi ulteriori punti d'inizio 
nei log WAL. Questa situazione non è un problema nella maggior parte delle situazioni 
dove potrebbe succedere. Generalmente, se il primario viene spento 
e quindi  non è più disponibile, probabilmente è per via di un fallimento grave
che richiede che lo standby venga convertito ad agire da nuovo primario in ogni caso. 
E in situazioni doe il primario viene arrestato intenzionalmente, assicurarsi  
che lo standby diventi il nuovo primario fa parte della procedura standard.
    </para>
   </listitem>
   <listitem>
    <para>
Al termine del ripristino, i lock di tipo <literal>AccessExclusiveLocks</literal> mantenuti dalle transazioni 
preparate richiederanno il doppio del normale numero di voci di lock di tabella. Se si pianifica 
di eseguire o un gran numero di transazioni preparate concorrenti rispetto 
a quelli presi normalmente, o pianificare di avere una transazione grande che prende molti 
<literal>AccessExclusiveLocks</literal>, si consiglia di selezionare un valore maggiore per 
<varname>max_locks_per_transaction</varname>, forse grande il doppio del valore 
del parametro sul server primario. Non si ha bisogno di preoccuparsi di questo 
se <varname>max_prepared_transactions</varname> è <literal>0</literal>.
    </para>
   </listitem>
  </itemizedlist>

   </para>
  </sect2>

 </sect1>

</chapter>
